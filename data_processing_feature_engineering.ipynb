{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717e3458",
   "metadata": {},
   "source": [
    "# Data Processing for Machine Learning: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c8f8f8",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4953806",
   "metadata": {},
   "source": [
    "\n",
    "### What is Feature Engineering?\n",
    "\n",
    "Feature engineering is the process of creating, modifying, or selecting features (variables) that improve the performance of a machine learning model. Good features can make a significant difference in model accuracy.\n",
    "\n",
    "### Types of Feature Engineering:\n",
    "\n",
    "1. **Feature Creation**: Creating new features from existing data.\n",
    "2. **Feature Transformation**: Transforming features (e.g., scaling, encoding).\n",
    "3. **Feature Selection**: Selecting the most important features for the model.\n",
    "4. **Dimensionality Reduction**: Reducing the number of features without losing important information.\n",
    "\n",
    "### Feature Creation\n",
    "\n",
    "Feature creation involves generating new features from existing ones to help the model capture hidden patterns. For example, you can create interaction terms between variables or use domain knowledge to create meaningful features.\n",
    "\n",
    "### Example: Creating Polynomial Features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Example: Creating polynomial features\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccae077",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Transformation\n",
    "\n",
    "Feature transformation involves modifying the values of features to improve model performance. This can include techniques like normalization, scaling, and encoding.\n",
    "\n",
    "#### Encoding Categorical Variables\n",
    "\n",
    "Categorical variables need to be transformed into numerical format for machine learning algorithms. This can be done using techniques like one-hot encoding.\n",
    "\n",
    "### Example: One-Hot Encoding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c7b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: One-hot encoding categorical variables using pandas\n",
    "df = pd.DataFrame({'Category': ['A', 'B', 'A', 'C', 'B']})\n",
    "df_encoded = pd.get_dummies(df, columns=['Category'])\n",
    "df_encoded\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8513a0f",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Selection\n",
    "\n",
    "Feature selection is the process of selecting the most relevant features for the model. It helps reduce overfitting, improve model interpretability, and speed up training.\n",
    "\n",
    "#### Example: Feature Selection Using Correlation\n",
    "\n",
    "We can use correlation to select features that are highly correlated with the target variable.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Feature selection using correlation\n",
    "import seaborn as sns\n",
    "\n",
    "# Creating a sample dataset\n",
    "df_selection = pd.DataFrame({'Feature1': [1, 2, 3, 4, 5],\n",
    "                             'Feature2': [10, 20, 30, 40, 50],\n",
    "                             'Target': [15, 25, 35, 45, 55]})\n",
    "\n",
    "# Visualizing correlation\n",
    "correlation_matrix = df_selection.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91277a96",
   "metadata": {},
   "source": [
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction techniques, like **Principal Component Analysis (PCA)**, reduce the number of features by combining them into a smaller set of new features that still capture most of the information.\n",
    "\n",
    "#### Example: Principal Component Analysis (PCA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example: Applying PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac990838",
   "metadata": {},
   "source": [
    "\n",
    "### Applications in Machine Learning\n",
    "\n",
    "- **Feature Creation** can enhance model performance by adding new informative features.\n",
    "- **Feature Transformation** ensures that data is in the correct format for modeling.\n",
    "- **Feature Selection** reduces the complexity of models by focusing on the most important variables.\n",
    "- **Dimensionality Reduction** helps reduce overfitting and speeds up training.\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
