{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f279a33d",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0018f6b5",
   "metadata": {},
   "source": [
    "## 1. Introduction to Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dfb4a2",
   "metadata": {},
   "source": [
    "\n",
    "### What are Recurrent Neural Networks (RNNs)?\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks that are specifically designed to process sequential data. Unlike traditional feedforward neural networks, RNNs have connections that form directed cycles, allowing them to maintain a memory of previous inputs. This makes RNNs particularly useful for tasks such as time series forecasting, natural language processing, and speech recognition.\n",
    "\n",
    "### Key Components of RNNs\n",
    "\n",
    "1. **Recurrent Layers**: These layers allow the network to remember information from previous time steps and use it to inform future predictions.\n",
    "2. **Hidden State**: The hidden state captures information from previous time steps and is updated at each step.\n",
    "3. **Output**: The output can be one value for each time step or a single value after processing the entire sequence.\n",
    "\n",
    "### Example: Simple RNN Using Keras\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Example: Creating a simple RNN\n",
    "rnn_model = Sequential([\n",
    "    SimpleRNN(50, activation='relu', input_shape=(10, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "rnn_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summary of the RNN model\n",
    "rnn_model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773201a7",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Types of RNN Architectures\n",
    "\n",
    "### Many-to-Many Architecture:\n",
    "- Used for tasks where both the input and output are sequences, such as machine translation or video classification.\n",
    "\n",
    "### Many-to-One Architecture:\n",
    "- Used for tasks where the input is a sequence, but the output is a single value, such as sentiment analysis or time series forecasting.\n",
    "\n",
    "## 3. Long Short-Term Memory (LSTM)\n",
    "\n",
    "LSTM is a type of RNN that is designed to overcome the limitations of traditional RNNs, such as the vanishing gradient problem. LSTMs have a more complex architecture that allows them to remember information for long periods.\n",
    "\n",
    "### Example: LSTM Using Keras\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fde250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Example: Creating an LSTM network\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(10, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "lstm_model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3075f73f",
   "metadata": {},
   "source": [
    "\n",
    "## Applications in Machine Learning\n",
    "\n",
    "- **RNNs** are used in sequential data processing tasks such as speech recognition, language modeling, and time series forecasting.\n",
    "- **LSTMs** are commonly used for tasks requiring long-term dependencies, such as machine translation and video classification.\n",
    "- **RNNs** and **LSTMs** are foundational models for natural language processing (NLP) tasks like text generation and sentiment analysis.\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
