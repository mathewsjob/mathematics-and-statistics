{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9711253",
   "metadata": {},
   "source": [
    "# Calculus for Machine Learning: Partial Derivatives and Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb56b1c",
   "metadata": {},
   "source": [
    "## 2. Partial Derivatives and Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ec60b1",
   "metadata": {},
   "source": [
    "\n",
    "### What are Partial Derivatives?\n",
    "\n",
    "Partial derivatives are used when we deal with functions of multiple variables. A partial derivative represents the rate of change of a function with respect to one variable while keeping all other variables constant.\n",
    "\n",
    "For example, given a function \\( f(x, y) \\), the partial derivative with respect to \\( x \\) is:\n",
    "\n",
    "\\[\n",
    "\f",
    "rac{\\partial f}{\\partial x}\n",
    "\\]\n",
    "\n",
    "And with respect to \\( y \\):\n",
    "\n",
    "\\[\n",
    "\f",
    "rac{\\partial f}{\\partial y}\n",
    "\\]\n",
    "\n",
    "### Example\n",
    "\n",
    "For the function \\( f(x, y) = x^2 + y^2 \\):\n",
    "\n",
    "\\[\n",
    "\f",
    "rac{\\partial f}{\\partial x} = 2x\n",
    "\\quad \text{and} \\quad\n",
    "\f",
    "rac{\\partial f}{\\partial y} = 2y\n",
    "\\]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bafffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Calculating partial derivatives using sympy\n",
    "x, y = sp.symbols('x y')\n",
    "f = x**2 + y**2\n",
    "\n",
    "partial_f_x = sp.diff(f, x)\n",
    "partial_f_y = sp.diff(f, y)\n",
    "\n",
    "partial_f_x, partial_f_y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbdebc3",
   "metadata": {},
   "source": [
    "\n",
    "### What is a Gradient?\n",
    "\n",
    "The gradient is a vector that contains all of the partial derivatives of a function with respect to its variables. It points in the direction of the greatest rate of increase of the function.\n",
    "\n",
    "For a function \\( f(x, y) \\), the gradient is given as:\n",
    "\n",
    "\\[\n",
    "\n",
    "abla f(x, y) = \\left( \f",
    "rac{\\partial f}{\\partial x}, \f",
    "rac{\\partial f}{\\partial y} \r",
    "ight)\n",
    "\\]\n",
    "\n",
    "In machine learning, the gradient is used in optimization algorithms like **gradient descent** to update the model parameters in the direction that minimizes the loss function.\n",
    "\n",
    "### Application in Machine Learning\n",
    "\n",
    "In machine learning, gradients are used to calculate the direction in which the model parameters (like weights in a neural network) should be updated to minimize the loss function. This process is called **gradient descent**.\n",
    "\n",
    "For example, if \\( L(w) \\) is the loss function with respect to model parameters \\( w \\), the gradient tells us how to update \\( w \\):\n",
    "\n",
    "\\[\n",
    "w = w - \\eta \n",
    "abla L(w)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\eta \\) is the learning rate.\n",
    "- \\( \n",
    "abla L(w) \\) is the gradient of the loss function with respect to \\( w \\).\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
