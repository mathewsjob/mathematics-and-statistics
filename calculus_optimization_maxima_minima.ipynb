{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81af4195",
   "metadata": {},
   "source": [
    "# Calculus for Machine Learning: Optimization Techniques (Maxima/Minima)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1a2483",
   "metadata": {},
   "source": [
    "## 5. Optimization Techniques (Maxima/Minima)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e24d34d",
   "metadata": {},
   "source": [
    "\n",
    "### What are Maxima and Minima?\n",
    "\n",
    "Maxima and minima (also called extrema) are the points where a function reaches its highest or lowest value, either locally or globally.\n",
    "\n",
    "- **Maximum (Maxima)**: A point at which the function has a higher value than at any nearby points.\n",
    "- **Minimum (Minima)**: A point at which the function has a lower value than at any nearby points.\n",
    "\n",
    "To find these points, we use the derivative of the function and set it to zero:\n",
    "\n",
    "\\[\n",
    "f'(x) = 0\n",
    "\\]\n",
    "\n",
    "Then we check the second derivative \\( f''(x) \\) to determine if the point is a maxima or minima:\n",
    "- If \\( f''(x) > 0 \\), it's a minimum.\n",
    "- If \\( f''(x) < 0 \\), it's a maximum.\n",
    "\n",
    "### Example: Finding Maxima/Minima\n",
    "Given a function \\( f(x) = x^3 - 3x^2 + 2 \\), find the critical points and determine if they are maxima or minima.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Finding maxima and minima using derivatives\n",
    "f = x**3 - 3*x**2 + 2\n",
    "\n",
    "# First derivative\n",
    "f_prime = sp.diff(f, x)\n",
    "\n",
    "# Second derivative\n",
    "f_double_prime = sp.diff(f_prime, x)\n",
    "\n",
    "# Solving for critical points (f'(x) = 0)\n",
    "critical_points = sp.solve(f_prime, x)\n",
    "\n",
    "# Evaluate the second derivative at critical points to determine maxima or minima\n",
    "critical_points, [f_double_prime.subs(x, cp) for cp in critical_points]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05d6a1",
   "metadata": {},
   "source": [
    "\n",
    "### Applications in Machine Learning\n",
    "\n",
    "Optimization is a key component in machine learning, especially for minimizing the loss function. The points of minima correspond to optimal solutions in algorithms like gradient descent.\n",
    "\n",
    "- **Global Minima**: The lowest point across the entire function.\n",
    "- **Local Minima**: The lowest point in a specific region of the function.\n",
    "  \n",
    "Gradient descent and other optimization techniques are used to find the local or global minima of the loss function, where the model performs best.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a8296",
   "metadata": {},
   "source": [
    "\n",
    "### Convex and Non-Convex Functions\n",
    "\n",
    "- **Convex Function**: A function that has only one global minimum. In machine learning, minimizing a convex loss function guarantees finding the global optimum.\n",
    "- **Non-Convex Function**: A function that may have multiple local minima. Algorithms like gradient descent can get stuck in local minima when minimizing non-convex functions.\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
