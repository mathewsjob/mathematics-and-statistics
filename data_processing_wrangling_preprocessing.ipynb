{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8701b13a",
   "metadata": {},
   "source": [
    "# Data Processing for Machine Learning: Data Wrangling and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb535d1",
   "metadata": {},
   "source": [
    "## 1. Data Wrangling and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bbdf7",
   "metadata": {},
   "source": [
    "\n",
    "### What is Data Wrangling?\n",
    "\n",
    "Data wrangling, also known as data preprocessing, is the process of cleaning, transforming, and preparing raw data for analysis. In machine learning, it is crucial to ensure that the data is in a suitable format for model building.\n",
    "\n",
    "### Common Steps in Data Wrangling:\n",
    "\n",
    "1. **Handling Missing Data**: Replace, remove, or impute missing values.\n",
    "2. **Handling Outliers**: Detect and address outliers that may distort the model.\n",
    "3. **Data Transformation**: Normalize, scale, or encode categorical features.\n",
    "4. **Feature Engineering**: Create new features or modify existing ones to improve model performance.\n",
    "5. **Data Splitting**: Split the dataset into training, validation, and test sets.\n",
    "\n",
    "### Example: Handling Missing Values\n",
    "\n",
    "We can use techniques like imputation to fill in missing values or drop rows/columns with missing data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c16b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example: Creating a DataFrame with missing values\n",
    "data = {'Feature1': [1, 2, np.nan, 4, 5],\n",
    "        'Feature2': [10, 20, 30, np.nan, 50]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Handling missing values by filling them with the mean\n",
    "df_filled = df.fillna(df.mean())\n",
    "df_filled\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256b4a8",
   "metadata": {},
   "source": [
    "\n",
    "### Handling Outliers\n",
    "\n",
    "Outliers are extreme values that deviate significantly from the rest of the data. We can detect outliers using statistical methods like Z-score or IQR (Interquartile Range).\n",
    "\n",
    "### Example: Detecting Outliers Using Z-Score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Example: Detecting outliers using Z-score\n",
    "data_outliers = np.array([1, 2, 3, 4, 5, 100])\n",
    "z_scores = np.abs(stats.zscore(data_outliers))\n",
    "outliers = np.where(z_scores > 2)\n",
    "outliers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc74e818",
   "metadata": {},
   "source": [
    "\n",
    "### Data Transformation\n",
    "\n",
    "Data transformation is often required to scale numerical features or encode categorical variables. For example, normalization scales values to a range between 0 and 1, while standardization centers the data around a mean of 0.\n",
    "\n",
    "### Example: Normalization and Standardization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Example: Normalizing and Standardizing data\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(df_filled)\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "data_standardized = scaler_standard.fit_transform(df_filled)\n",
    "\n",
    "data_normalized, data_standardized\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac487e",
   "metadata": {},
   "source": [
    "\n",
    "### Data Splitting\n",
    "\n",
    "It is crucial to split your dataset into training, validation, and test sets. The training set is used to train the model, the validation set is used for model tuning, and the test set is used to evaluate final model performance.\n",
    "\n",
    "### Example: Splitting Data into Train and Test Sets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7865de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example: Splitting data into training and test sets\n",
    "X = df_filled[['Feature1', 'Feature2']]\n",
    "y = np.array([1, 0, 1, 0, 1])  # Example labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b29c2",
   "metadata": {},
   "source": [
    "\n",
    "### Applications in Machine Learning\n",
    "\n",
    "- **Data Wrangling** is essential for cleaning and preparing data before model training.\n",
    "- Proper **handling of missing data** and **outliers** can significantly improve model performance.\n",
    "- **Data transformation** ensures features are in the right format for machine learning models.\n",
    "- **Data splitting** helps assess model performance on unseen data.\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
