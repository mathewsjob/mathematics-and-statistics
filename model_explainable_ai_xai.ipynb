{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b712a225",
   "metadata": {},
   "source": [
    "# Explainable AI (XAI) for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f8163",
   "metadata": {},
   "source": [
    "## 1. Introduction to Explainable AI (XAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05d1c7",
   "metadata": {},
   "source": [
    "\n",
    "### What is Explainable AI (XAI)?\n",
    "\n",
    "Explainable AI (XAI) refers to a set of techniques that make the behavior and predictions of machine learning models more interpretable and understandable to humans. In many applications, such as healthcare or finance, it is essential to understand **why** a model made a certain prediction, not just what the prediction is.\n",
    "\n",
    "XAI techniques help to:\n",
    "1. Increase trust in machine learning models.\n",
    "2. Ensure models follow ethical and legal standards.\n",
    "3. Improve model debugging and transparency.\n",
    "\n",
    "### Key Techniques in XAI\n",
    "\n",
    "1. **SHAP (SHapley Additive exPlanations)**: A game theory-based approach that assigns a contribution value to each feature based on its impact on the prediction.\n",
    "2. **LIME (Local Interpretable Model-agnostic Explanations)**: Approximates complex models locally to understand predictions on specific instances.\n",
    "3. **Feature Importance**: Ranks features by their contribution to the modelâ€™s predictions.\n",
    "\n",
    "### Example: Feature Importance in Decision Trees\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f260b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Example: Training a RandomForest model on the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "importances\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa5572",
   "metadata": {},
   "source": [
    "\n",
    "## 2. SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "SHAP values explain the contribution of each feature to the final prediction. SHAP is model-agnostic, meaning it can be applied to any machine learning model.\n",
    "\n",
    "### Example: Using SHAP to Explain Model Predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346106e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "\n",
    "# Compute SHAP values for the first instance\n",
    "shap_values = explainer.shap_values(X[0:1])\n",
    "\n",
    "# Plot SHAP values\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], features=X[0], feature_names=data.feature_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99737b3",
   "metadata": {},
   "source": [
    "\n",
    "## 3. LIME (Local Interpretable Model-agnostic Explanations)\n",
    "\n",
    "LIME provides local explanations for individual predictions by approximating the decision boundary of a complex model with an interpretable surrogate model (e.g., linear model) in the vicinity of a specific instance.\n",
    "\n",
    "### Example: Using LIME to Explain Predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Using LIME (skipping detailed implementation for simplicity)\n",
    "# LIME provides local explanations for individual instances.\n",
    "# You can use the lime package to generate explanations for complex models.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4652b6e1",
   "metadata": {},
   "source": [
    "\n",
    "## Applications of XAI\n",
    "\n",
    "- **Model Transparency**: XAI techniques help explain why models make certain predictions, which is critical in high-stakes domains like healthcare and finance.\n",
    "- **Ethical AI**: Ensuring models are fair, accountable, and ethical by identifying and mitigating bias.\n",
    "- **Debugging and Trust**: By making models interpretable, XAI techniques improve debugging and increase trust in AI systems.\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
