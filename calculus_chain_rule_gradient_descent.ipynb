{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa310aa",
   "metadata": {},
   "source": [
    "# Calculus for Machine Learning: Chain Rule and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02111e5",
   "metadata": {},
   "source": [
    "## 3. Chain Rule and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87975a",
   "metadata": {},
   "source": [
    "\n",
    "### Chain Rule\n",
    "\n",
    "The chain rule is a formula for computing the derivative of a composite function. If a variable \\( z \\) depends on \\( y \\), and \\( y \\) depends on \\( x \\), then the derivative of \\( z \\) with respect to \\( x \\) can be calculated as:\n",
    "\n",
    "\\[\n",
    "\f",
    "rac{dz}{dx} = \f",
    "rac{dz}{dy} \\cdot \f",
    "rac{dy}{dx}\n",
    "\\]\n",
    "\n",
    "This rule is important in machine learning, especially in backpropagation for training neural networks, where the loss function is a composite function of the model parameters.\n",
    "\n",
    "### Example\n",
    "\n",
    "Given \\( z = (3x^2 + 2x + 1)^2 \\), we can apply the chain rule to find the derivative of \\( z \\) with respect to \\( x \\).\n",
    "\n",
    "Step 1: Let \\( y = 3x^2 + 2x + 1 \\), so \\( z = y^2 \\).\n",
    "\n",
    "Step 2: Apply the chain rule:\n",
    "\n",
    "\\[\n",
    "\f",
    "rac{dz}{dx} = \f",
    "rac{dz}{dy} \\cdot \f",
    "rac{dy}{dx} = 2y \\cdot (6x + 2)\n",
    "\\]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Calculating derivative using the chain rule\n",
    "y = 3*x**2 + 2*x + 1\n",
    "z = y**2\n",
    "\n",
    "dz_dx = sp.diff(z, x)\n",
    "dz_dx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c6c18",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the loss function by iteratively updating the model parameters in the direction of the negative gradient. The update rule for gradient descent is:\n",
    "\n",
    "\\[\n",
    "w = w - \\eta \n",
    "abla L(w)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( w \\) is the model parameter.\n",
    "- \\( \\eta \\) is the learning rate (step size).\n",
    "- \\( \n",
    "abla L(w) \\) is the gradient of the loss function with respect to \\( w \\).\n",
    "\n",
    "#### Steps in Gradient Descent:\n",
    "1. **Initialize**: Start with initial values for the model parameters.\n",
    "2. **Calculate Gradient**: Compute the gradient of the loss function with respect to each parameter.\n",
    "3. **Update Parameters**: Update the parameters by moving them in the opposite direction of the gradient.\n",
    "4. **Repeat**: Repeat the process until convergence (when the parameters no longer change significantly).\n",
    "\n",
    "### Example: Linear Regression using Gradient Descent\n",
    "\n",
    "In linear regression, we minimize the Mean Squared Error (MSE) between the predicted values and the actual values. The gradient of the MSE with respect to the weights is used to update the weights during training.\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
